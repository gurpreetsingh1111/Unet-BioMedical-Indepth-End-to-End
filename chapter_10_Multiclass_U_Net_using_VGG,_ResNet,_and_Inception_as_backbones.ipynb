{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_10_Multiclass U-Net using VGG, ResNet, and Inception as backbones",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Multiclass semantic segmentation using U-Net with VGG, ResNet, and Inception \n",
        "as backbones\n",
        "Segmentation models: https://github.com/qubvel/segmentation_models\n",
        "To annotate images and generate labels, you can use APEER (for free):\n",
        "www.apeer.com \n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import segmentation_models as sm\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import keras \n",
        "\n",
        "from keras.utils import normalize\n",
        "from keras.metrics import MeanIoU\n",
        "\n",
        "\n",
        "#Resizing images, if needed\n",
        "SIZE_X = 128 \n",
        "SIZE_Y = 128\n",
        "n_classes=4 #Number of classes for segmentation\n",
        "\n",
        "#Capture training image info as a list\n",
        "train_images = []\n",
        "\n",
        "for directory_path in glob.glob(\"128_patches/images/\"):\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
        "        img = cv2.imread(img_path, 1)       \n",
        "        #img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
        "        train_images.append(img)\n",
        "       \n",
        "#Convert list to array for machine learning processing        \n",
        "train_images = np.array(train_images)\n",
        "\n",
        "#Capture mask/label info as a list\n",
        "train_masks = [] \n",
        "for directory_path in glob.glob(\"128_patches/masks/\"):\n",
        "    for mask_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
        "        mask = cv2.imread(mask_path, 0)       \n",
        "        #mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
        "        train_masks.append(mask)\n",
        "        \n",
        "#Convert list to array for machine learning processing          \n",
        "train_masks = np.array(train_masks)\n",
        "\n",
        "###############################################\n",
        "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "n, h, w = train_masks.shape\n",
        "train_masks_reshaped = train_masks.reshape(-1,1)\n",
        "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
        "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "np.unique(train_masks_encoded_original_shape)\n",
        "\n",
        "#################################################\n",
        "#train_images = np.expand_dims(train_images, axis=3)\n",
        "#train_images = normalize(train_images, axis=1)\n",
        "\n",
        "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)# (1600,128,128,1)\n",
        "\n",
        "#Create a subset of data for quick testing\n",
        "#Picking 10% for testing and remaining for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
        "\n",
        "#Further split training data t a smaller subset for quick testing of models\n",
        "X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.5, random_state = 0)\n",
        "\n",
        "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \n",
        "\n",
        "from keras.utils import to_categorical\n",
        "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
        "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
        "\n",
        "\n",
        "\n",
        "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
        "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))#(160,128,128,4)\n",
        "\n",
        "######################################################\n",
        "#Reused parameters in all models\n",
        "\n",
        "n_classes=4\n",
        "activation='softmax'\n",
        "\n",
        "LR = 0.0001\n",
        "optim = keras.optimizers.Adam(LR)\n",
        "\n",
        "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
        "# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
        "dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.25, 0.25, 0.25, 0.25])) \n",
        "focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "total_loss = dice_loss + (1 * focal_loss)\n",
        "\n",
        "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
        "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
        "\n",
        "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
        "\n",
        "\n",
        "########################################################################\n",
        "###Model 1\n",
        "BACKBONE1 = 'resnet34'\n",
        "preprocess_input1 = sm.get_preprocessing(BACKBONE1)\n",
        "\n",
        "# preprocess input\n",
        "X_train1 = preprocess_input1(X_train)\n",
        "X_test1 = preprocess_input1(X_test)\n",
        "\n",
        "# define model\n",
        "model1 = sm.Unet(BACKBONE1, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model1.compile(optim, total_loss, metrics=metrics)\n",
        "\n",
        "#model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "\n",
        "print(model1.summary())\n",
        "\n",
        "\n",
        "history1=model1.fit(X_train1, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test1, y_test_cat))\n",
        "\n",
        "\n",
        "model1.save('res34_backbone_50epochs.hdf5')\n",
        "############################################################\n",
        "###Model 2\n",
        "\n",
        "BACKBONE2 = 'inceptionv3'\n",
        "preprocess_input2 = sm.get_preprocessing(BACKBONE2)\n",
        "\n",
        "# preprocess input\n",
        "X_train2 = preprocess_input2(X_train)\n",
        "X_test2 = preprocess_input2(X_test)\n",
        "\n",
        "# define model\n",
        "model2 = sm.Unet(BACKBONE2, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model2.compile(optim, total_loss, metrics)\n",
        "#model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "\n",
        "\n",
        "print(model2.summary())\n",
        "\n",
        "\n",
        "history2=model2.fit(X_train2, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test2, y_test_cat))\n",
        "\n",
        "\n",
        "model2.save('inceptionv3_backbone_50epochs.hdf5')\n",
        "\n",
        "#####################################################\n",
        "###Model 3\n",
        "\n",
        "BACKBONE3 = 'vgg16'\n",
        "preprocess_input3 = sm.get_preprocessing(BACKBONE3)\n",
        "\n",
        "# preprocess input\n",
        "X_train3 = preprocess_input3(X_train)\n",
        "X_test3 = preprocess_input3(X_test)\n",
        "\n",
        "\n",
        "# define model\n",
        "model3 = sm.Unet(BACKBONE3, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model3.compile(optim, total_loss, metrics)\n",
        "#model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "\n",
        "\n",
        "print(model3.summary())\n",
        "\n",
        "history3=model3.fit(X_train3, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test3, y_test_cat))\n",
        "\n",
        "\n",
        "model3.save('vgg19_backbone_50epochs.hdf5')\n",
        "\n",
        "\n",
        "##########################################################\n",
        "\n",
        "###\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history1.history['loss']\n",
        "val_loss = history1.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history1.history['iou_score']\n",
        "val_acc = history1.history['val_iou_score']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation IOU')\n",
        "plt.title('Training and validation IOU')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IOU')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#####################################################\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "### FOR NOW LET US FOCUS ON A SINGLE MODEL\n",
        "\n",
        "#Set compile=False as we are not loading it for training, only for prediction.\n",
        "model1 = load_model('saved_models/res34_backbone_50epochs.hdf5', compile=False)\n",
        "model2 = load_model('saved_models/inceptionv3_backbone_50epochs.hdf5', compile=False)\n",
        "model3 = load_model('saved_models/vgg19_backbone_50epochs.hdf5', compile=False)\n",
        "\n",
        "#IOU\n",
        "y_pred1=model2.predict(X_test2)\n",
        "y_pred1_argmax=np.argmax(y_pred1, axis=3)#(160,128,128)\n",
        "\n",
        "\n",
        "#Using built in keras function\n",
        "#from keras.metrics import MeanIoU\n",
        "n_classes = 4\n",
        "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
        "IOU_keras.update_state(y_test[:,:,:,0], y_pred1_argmax)\n",
        "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
        "\n",
        "\n",
        "#To calculate I0U for each class...\n",
        "values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
        "print(values)\n",
        "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
        "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
        "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
        "class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
        "\n",
        "print(\"IoU for class1 is: \", class1_IoU)\n",
        "print(\"IoU for class2 is: \", class2_IoU)\n",
        "print(\"IoU for class3 is: \", class3_IoU)\n",
        "print(\"IoU for class4 is: \", class4_IoU)\n",
        "\n",
        "#Vaerify the prediction on first image\n",
        "plt.imshow(train_images[0, :,:,0], cmap='gray')\n",
        "plt.imshow(train_masks[0], cmap='gray')\n",
        "##############################################################\n",
        "\n",
        "# Test some random images\n",
        "import random\n",
        "test_img_number = random.randint(0, len(X_test2))\n",
        "test_img = X_test2[test_img_number]\n",
        "ground_truth=y_test[test_img_number]\n",
        "test_img_input=np.expand_dims(test_img, 0)\n",
        "\n",
        "test_img_input1 = preprocess_input2(test_img_input)\n",
        "\n",
        "test_pred1 = model2.predict(test_img_input1)\n",
        "test_prediction1 = np.argmax(test_pred1, axis=3)[0,:,:]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img[:,:,0], cmap='gray')\n",
        "plt.subplot(232)\n",
        "plt.title('Testing Label')\n",
        "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
        "plt.subplot(233)\n",
        "plt.title('Prediction on test image')\n",
        "plt.imshow(test_prediction1, cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yxLAAqqgsVtJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}