{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_7 Using IoU (Jaccard) as loss function to train U-Net for semantic segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)  # -1 ultiplied as we want to minimize this value as loss function\n",
        "\n",
        "################################################################\n",
        "def simple_unet_model_with_jacard(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
        "#Build the model\n",
        "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
        "    s = inputs\n",
        "\n",
        "    #Contraction path\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
        "    c1 = Dropout(0.1)(c1)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "    \n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = Dropout(0.1)(c2)\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "     \n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "     \n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = Dropout(0.2)(c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "     \n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "    \n",
        "    #Expansive path \n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = Dropout(0.2)(c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "     \n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = Dropout(0.2)(c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "     \n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "    c8 = Dropout(0.1)(c8)\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "     \n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "    c9 = Dropout(0.1)(c9)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "     \n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "     \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.compile(optimizer = 'adam', loss = [jacard_coef_loss], metrics = [jacard_coef])\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "-W849CXX3BHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training and testing for semantic segmentation (Unet) of mitochondria\n",
        "Uses standard Unet framework with Jacard for loss. \n",
        "Dataset info: Electron microscopy (EM) dataset from\n",
        "https://www.epfl.ch/labs/cvlab/data/data-em/\n",
        "Patches of 256x256 from images and labels \n",
        "have been extracted (via separate program) and saved to disk. \n",
        "This code uses 256x256 images/masks.\n",
        "\"\"\"\n",
        "\n",
        "from simple_unet_model_with_jacard import simple_unet_model_with_jacard   #Use normal unet model\n",
        "from simple_unet_model import simple_unet_model   #Use normal unet model\n",
        "\n",
        "from keras.utils import normalize\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "image_directory = 'data/generated_patches/images/'\n",
        "mask_directory = 'data/generated_patches/masks/'\n",
        "\n",
        "\n",
        "SIZE = 256\n",
        "image_dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
        "mask_dataset = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n",
        "\n",
        "images = os.listdir(image_directory)\n",
        "for i, image_name in enumerate(images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
        "    if (image_name.split('.')[1] == 'tif'):\n",
        "        #print(image_directory+image_name)\n",
        "        image = cv2.imread(image_directory+image_name, 0)\n",
        "        image = Image.fromarray(image)\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        image_dataset.append(np.array(image))\n",
        "\n",
        "#Iterate through all images in Uninfected folder, resize to 64 x 64\n",
        "#Then save into the same numpy array 'dataset' but with label 1\n",
        "\n",
        "masks = os.listdir(mask_directory)\n",
        "for i, image_name in enumerate(masks):\n",
        "    if (image_name.split('.')[1] == 'tif'):\n",
        "        image = cv2.imread(mask_directory+image_name, 0)\n",
        "        image = Image.fromarray(image)\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        mask_dataset.append(np.array(image))\n",
        "\n",
        "\n",
        "#Normalize images\n",
        "# i am not normalize mask because my mask have 0 or 255 values \n",
        "image_dataset = np.expand_dims(normalize(np.array(image_dataset), axis=1),3)\n",
        "#D not normalize masks, just rescale to 0 to 1.\n",
        "mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255.\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.10, random_state = 0)\n",
        "\n",
        "X_train_quick_test, X_test_quick_test, y_train_quick_test, y_test_quick_test = train_test_split(X_train, y_train, test_size = 0.9, random_state = 0)\n",
        "\n",
        "#Sanity check, view few mages\n",
        "import random\n",
        "import numpy as np\n",
        "image_number = random.randint(0, len(X_train_quick_test))\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(121)\n",
        "plt.imshow(np.reshape(X_train_quick_test[image_number], (256, 256)), cmap='gray')\n",
        "plt.subplot(122)\n",
        "plt.imshow(np.reshape(y_train_quick_test[image_number], (256, 256)), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "###############################################################\n",
        "IMG_HEIGHT = image_dataset.shape[1]\n",
        "IMG_WIDTH  = image_dataset.shape[2]\n",
        "IMG_CHANNELS = image_dataset.shape[3]\n",
        "\n",
        "def get_jacard_model():\n",
        "    return simple_unet_model_with_jacard(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "\n",
        "model_jacard = get_jacard_model()\n",
        "\n",
        "def get_standard_model():\n",
        "    return simple_unet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "\n",
        "model_standard = get_standard_model()\n",
        "\n",
        "#If starting with pre-trained weights. \n",
        "#model.load_weights('mitochondria_with_jacard_50_epochs.hdf5')\n",
        "\n",
        "history_jacard = model_jacard.fit(X_train_quick_test, y_train_quick_test, \n",
        "                    batch_size = 16, \n",
        "                    verbose=1, \n",
        "                    epochs=10, \n",
        "                    validation_data=(X_test, y_test), \n",
        "                    shuffle=False)\n",
        "\n",
        "history_standard = model_standard.fit(X_train_quick_test, y_train_quick_test, \n",
        "                    batch_size = 16, \n",
        "                    verbose=1, \n",
        "                    epochs=10, \n",
        "                    validation_data=(X_test, y_test), \n",
        "                    shuffle=False)\n",
        "\n",
        "model_jacard.save('mitochondria_with_jacard.hdf5')\n",
        "model_standard.save('mitochondria_standard.hdf5')\n",
        "#To be compared with trained model mitochondria_gpu_tf1.4.hdf5\n",
        "# that just used cross entropy for loss and accuracy for metric. \n",
        "\n",
        "############################################################\n",
        "#Evaluate the model\n",
        "\n",
        "\n",
        "\t# evaluate model\n",
        "_, acc = model_jacard.evaluate(X_test, y_test)\n",
        "print(\"Accuracy of Jacard Model is = \", (acc * 100.0), \"%\")\n",
        "\n",
        "\t# evaluate model\n",
        "_, acc = model_standard.evaluate(X_test, y_test)\n",
        "print(\"Accuracy of Standard Model is = \", (acc * 100.0), \"%\")\n",
        "\n",
        "\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history_jacard.history['loss']\n",
        "val_loss = history_jacard.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "jc = history_jacard.history['jacard_coef']\n",
        "#acc = history.history['accuracy']\n",
        "val_jc = history_jacard.history['val_jacard_coef']\n",
        "#val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, jc, 'y', label='Training Jacard Coeff.')\n",
        "plt.plot(epochs, val_jc, 'r', label='Validation Jacard Coeff.')\n",
        "plt.title('Training and validation Jacard')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Jacard Coefficient')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "###\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history_standard.history['loss']\n",
        "val_loss = history_standard.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history_standard.history['acc']\n",
        "#acc = history.history['accuracy']\n",
        "val_acc = history_standard.history['val_acc']\n",
        "#val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "##################################\n",
        "\n",
        "model = model_jacard  #Assign model to one of the models instead of changing the entire code for testing. \n",
        "#model = model_standard  #Assign model to one of the models instead of changing the entire code for testing. \n",
        "\n",
        "#IOU\n",
        "y_pred=model.predict(X_test)\n",
        "y_pred_thresholded = y_pred > 0.5\n",
        "\n",
        "intersection = np.logical_and(y_test, y_pred_thresholded)\n",
        "union = np.logical_or(y_test, y_pred_thresholded)\n",
        "iou_score = np.sum(intersection) / np.sum(union)\n",
        "print(\"IoU socre is: \", iou_score)\n",
        "\n",
        "#######################################################################\n",
        "#Predict on a few images\n",
        "#model = get_model()\n",
        "#model.load_weights('mitochondria_with_jacard_50_plus_50_epochs.hdf5') #Trained for 50 epochs and then additional 100\n",
        "#model.load_weights('mitochondria_gpu_tf1.4.hdf5')  #Trained for 50 epochs\n",
        "\n",
        "test_img_number = random.randint(0, len(X_test))\n",
        "test_img = X_test[test_img_number]\n",
        "ground_truth=y_test[test_img_number]\n",
        "test_img_norm=test_img[:,:,0][:,:,None]\n",
        "test_img_input=np.expand_dims(test_img_norm, 0)\n",
        "prediction = (model.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
        "\n",
        "test_img_other = cv2.imread('data/test_images/01-1_256.tif', 0)\n",
        "test_img_other_norm = np.expand_dims(normalize(np.array(test_img_other), axis=1),2)\n",
        "test_img_other_norm=test_img_other_norm[:,:,0][:,:,None]\n",
        "test_img_other_input=np.expand_dims(test_img_other_norm, 0)\n",
        "\n",
        "#Predict and threshold for values above 0.5 probability\n",
        "prediction_other = (model.predict(test_img_other_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img[:,:,0], cmap='gray')\n",
        "plt.subplot(232)\n",
        "plt.title('Testing Label')\n",
        "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
        "plt.subplot(233)\n",
        "plt.title('Prediction on test image')\n",
        "plt.imshow(prediction, cmap='gray')\n",
        "plt.subplot(234)\n",
        "plt.title('External Image')\n",
        "plt.imshow(test_img_other, cmap='gray')\n",
        "plt.subplot(235)\n",
        "plt.title('Prediction of external Image')\n",
        "plt.imshow(prediction_other, cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "#plt.imsave('input.jpg', test_img[:,:,0], cmap='gray')\n",
        "plt.imsave('data/results/output2.jpg', prediction_other, cmap='gray')\n"
      ],
      "metadata": {
        "id": "QtMZBZUu3BJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZIcj3Dpv3BMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "luU-VKZE3BPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v9NvSTzH3BTE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}