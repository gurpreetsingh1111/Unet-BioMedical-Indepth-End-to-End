{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_21_Semantic Segmentation of Landcover Dataset using U-Net.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LandCover.ai.url"
      ],
      "metadata": {
        "id": "vFD056AgqxN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [InternetShortcut]\n",
        "# URL=https://landcover.ai/"
      ],
      "metadata": {
        "id": "fK3D7zCbqxTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##landcover_prepare_data.py"
      ],
      "metadata": {
        "id": "DgwQ5BOPqxV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Author: Dr. Sreenivas Bhattiprolu \n",
        "The following code performs these tasks - relevant to work with landcover dataset\n",
        "from here: https://landcover.ai/\n",
        "Code can be modified to work with any other dataset.\n",
        "Tasks achieved.\n",
        "1. Read large images and corresponding masks, divide them into smaller patches.\n",
        "And write the patches as images to the local drive.  \n",
        "2. Save only images and masks where masks have some decent amount of labels other than 0. \n",
        "Using blank images with label=0 is a waste of time and may bias the model towards \n",
        "unlabeled pixels. \n",
        "3. Divide the sorted dataset from above into train and validation datasets. \n",
        "4. You have to manually move some folders and rename appropriately if you want to use \n",
        "ImageDataGenerator from keras. \n",
        "\"\"\"\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from patchify import patchify\n",
        "import tifffile as tiff\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import segmentation_models as sm\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "import random\n",
        "\n",
        "#Quick understanding of the dataset\n",
        "temp_img = cv2.imread(\"data/images/M-34-51-C-d-4-1.tif\") #3 channels / spectral bands\n",
        "plt.imshow(temp_img[:,:,2]) #View each channel...\n",
        "temp_mask = cv2.imread(\"data/masks/M-34-51-C-d-4-1.tif\") #3 channels but all same. \n",
        "labels, count = np.unique(temp_mask[:,:,0], return_counts=True) #Check for each channel. All chanels are identical\n",
        "print(\"Labels are: \", labels, \" and the counts are: \", count)\n",
        "\n",
        "#Now, crop each large image into patches of 256x256. Save them into a directory \n",
        "#so we can use data augmentation and read directly from the drive. \n",
        "root_directory = 'data/'\n",
        "\n",
        "patch_size = 256\n",
        "\n",
        "#Read images from repsective 'images' subdirectory\n",
        "#As all images are of different size we have 2 options, either resize or crop\n",
        "#But, some images are too large and some small. Resizing will change the size of real objects.\n",
        "#Therefore, we will crop them to a nearest size divisible by 256 and then \n",
        "#divide all images into patches of 256x256x3. \n",
        "img_dir=root_directory+\"images/\"\n",
        "for path, subdirs, files in os.walk(img_dir):\n",
        "    #print(path)  \n",
        "    dirname = path.split(os.path.sep)[-1]\n",
        "    #print(dirname)\n",
        "    images = os.listdir(path)  #List of all image names in this subdirectory\n",
        "    #print(images)\n",
        "    for i, image_name in enumerate(images):  \n",
        "        if image_name.endswith(\".tif\"):\n",
        "            #print(image_name)\n",
        "            image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n",
        "            SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
        "            SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
        "            image = Image.fromarray(image)\n",
        "            image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
        "            #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
        "            image = np.array(image)             \n",
        "   \n",
        "            #Extract patches from each image\n",
        "            print(\"Now patchifying image:\", path+\"/\"+image_name)\n",
        "            patches_img = patchify(image, (256, 256, 3), step=256)  #Step=256 for 256 patches means no overlap\n",
        "    \n",
        "            for i in range(patches_img.shape[0]):\n",
        "                for j in range(patches_img.shape[1]):\n",
        "                    \n",
        "                    single_patch_img = patches_img[i,j,:,:]\n",
        "                    #single_patch_img = (single_patch_img.astype('float32')) / 255. #We will preprocess using one of the backbones\n",
        "                    single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
        "                    \n",
        "                    cv2.imwrite(root_directory+\"256_patches/images/\"+\n",
        "                               image_name+\"patch_\"+str(i)+str(j)+\".tif\", single_patch_img)\n",
        "                    #image_dataset.append(single_patch_img)\n",
        "            \n",
        "  \n",
        " #Now do the same as above for masks\n",
        " #For this specific dataset we could have added masks to the above code as masks have extension png\n",
        "mask_dir=root_directory+\"masks/\"\n",
        "for path, subdirs, files in os.walk(mask_dir):\n",
        "    #print(path)  \n",
        "    dirname = path.split(os.path.sep)[-1]\n",
        "\n",
        "    masks = os.listdir(path)  #List of all image names in this subdirectory\n",
        "    for i, mask_name in enumerate(masks):  \n",
        "        if mask_name.endswith(\".tif\"):           \n",
        "            mask = cv2.imread(path+\"/\"+mask_name, 0)  #Read each image as Grey (or color but remember to map each color to an integer)\n",
        "            SIZE_X = (mask.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
        "            SIZE_Y = (mask.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
        "            mask = Image.fromarray(mask)\n",
        "            mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
        "            #mask = mask.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
        "            mask = np.array(mask)             \n",
        "   \n",
        "            #Extract patches from each image\n",
        "            print(\"Now patchifying mask:\", path+\"/\"+mask_name)\n",
        "            patches_mask = patchify(mask, (256, 256), step=256)  #Step=256 for 256 patches means no overlap\n",
        "    \n",
        "            for i in range(patches_mask.shape[0]):\n",
        "                for j in range(patches_mask.shape[1]):\n",
        "                    \n",
        "                    single_patch_mask = patches_mask[i,j,:,:]\n",
        "                    #single_patch_img = (single_patch_img.astype('float32')) / 255. #No need to scale masks, but you can do it if you want\n",
        "                    #single_patch_mask = single_patch_mask[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
        "                    cv2.imwrite(root_directory+\"256_patches/masks/\"+\n",
        "                               mask_name+\"patch_\"+str(i)+str(j)+\".tif\", single_patch_mask)\n",
        "\n",
        "\n",
        "\n",
        "train_img_dir = \"data/256_patches/images/\"\n",
        "train_mask_dir = \"data/256_patches/masks/\"\n",
        "\n",
        "img_list = os.listdir(train_img_dir)\n",
        "msk_list = os.listdir(train_mask_dir)\n",
        "\n",
        "num_images = len(os.listdir(train_img_dir))\n",
        "\n",
        "\n",
        "img_num = random.randint(0, num_images-1)\n",
        "\n",
        "img_for_plot = cv2.imread(train_img_dir+img_list[img_num], 1)\n",
        "img_for_plot = cv2.cvtColor(img_for_plot, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "mask_for_plot =cv2.imread(train_mask_dir+msk_list[img_num], 0)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(121)\n",
        "plt.imshow(img_for_plot)\n",
        "plt.title('Image')\n",
        "plt.subplot(122)\n",
        "plt.imshow(mask_for_plot, cmap='gray')\n",
        "plt.title('Mask')\n",
        "plt.show()\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "#Now, let us copy images and masks with real information to a new folder.\n",
        "# real information = if mask has decent amount of labels other than 0. \n",
        "\n",
        "useless=0  #Useless image counter\n",
        "for img in range(len(img_list)):   #Using t1_list as all lists are of same size\n",
        "    img_name=img_list[img]\n",
        "    mask_name = msk_list[img]\n",
        "    print(\"Now preparing image and masks number: \", img)\n",
        "      \n",
        "    temp_image=cv2.imread(train_img_dir+img_list[img], 1)\n",
        "   \n",
        "    temp_mask=cv2.imread(train_mask_dir+msk_list[img], 0)\n",
        "    #temp_mask=temp_mask.astype(np.uint8)\n",
        "    \n",
        "    val, counts = np.unique(temp_mask, return_counts=True)\n",
        "    \n",
        "    if (1 - (counts[0]/counts.sum())) > 0.05:  #At least 5% useful area with labels that are not 0\n",
        "        print(\"Save Me\")\n",
        "        cv2.imwrite('data/256_patches/images_with_useful_info/images/'+img_name, temp_image)\n",
        "        cv2.imwrite('data/256_patches/images_with_useful_info/masks/'+mask_name, temp_mask)\n",
        "        \n",
        "    else:\n",
        "        print(\"I am useless\")   \n",
        "        useless +=1\n",
        "\n",
        "print(\"Total useful images are: \", len(img_list)-useless)  #20,075\n",
        "print(\"Total useless images are: \", useless) #21,571\n",
        "###############################################################\n",
        "#Now split the data into training, validation and testing. \n",
        "\n",
        "\"\"\"\n",
        "Code for splitting folder into train, test, and val.\n",
        "Once the new folders are created rename them and arrange in the format below to be used\n",
        "for semantic segmentation using data generators. \n",
        "pip install split-folders\n",
        "\"\"\"\n",
        "import splitfolders  # or import split_folders\n",
        "\n",
        "input_folder = 'data/256_patches/images_with_useful_info/'\n",
        "output_folder = 'data/data_for_training_and_testing/'\n",
        "# Split with a ratio.\n",
        "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
        "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.75, .25), group_prefix=None) # default values\n",
        "########################################\n",
        "\n",
        "#Now manually move folders around to bring them to the following structure.\n",
        "\"\"\"\n",
        "Your current directory structure:\n",
        "Data/\n",
        "    train/\n",
        "        images/\n",
        "            img1, img2, ...\n",
        "        masks/\n",
        "            msk1, msk2, ....\n",
        "    val/\n",
        "        images/\n",
        "            img1, img2, ...\n",
        "        masks/\n",
        "            msk1, msk2, ....\n",
        "        \n",
        "Copy the folders around to the following structure... \n",
        "Data/\n",
        "    train_images/\n",
        "                train/\n",
        "                    img1, img2, img3, ......\n",
        "    \n",
        "    train_masks/\n",
        "                train/\n",
        "                    msk1, msk, msk3, ......\n",
        "                    \n",
        "    val_images/\n",
        "                val/\n",
        "                    img1, img2, img3, ......                \n",
        "    val_masks/\n",
        "                val/\n",
        "                    msk1, msk, msk3, ......\n",
        "      \n",
        "                    \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UR8sPTSLqxYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##prediction_using_smooth_blending.py"
      ],
      "metadata": {
        "id": "wbM2_qI1qxbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from patchify import patchify, unpatchify\n",
        "from PIL import Image\n",
        "import segmentation_models as sm\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "from smooth_tiled_predictions import predict_img_with_smooth_windowing\n",
        "\n",
        "BACKBONE = 'resnet34'\n",
        "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
        "\n",
        "img = cv2.imread(\"data/images/N-34-66-C-c-4-3.tif\")  #N-34-66-C-c-4-3.tif, N-34-97-D-c-2-4.tif\n",
        "input_img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n",
        "input_img = preprocess_input(input_img)\n",
        "\n",
        "original_mask = cv2.imread(\"data/masks/N-34-66-C-c-4-3.tif\")\n",
        "original_mask = original_mask[:,:,0]  #Use only single channel...\n",
        "#original_mask = to_categorical(original_mask, num_classes=n_classes)\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model(\"landcover_25_epochs_RESNET_backbone_batch16.hdf5\", compile=False)\n",
        "                  \n",
        "# size of patches\n",
        "patch_size = 256\n",
        "\n",
        "# Number of classes \n",
        "n_classes = 4\n",
        "\n",
        "         \n",
        "###################################################################################\n",
        "#Predict using smooth blending\n",
        "\n",
        "# Use the algorithm. The `pred_func` is passed and will process all the image 8-fold by tiling small patches with overlap, called once with all those image as a batch outer dimension.\n",
        "# Note that model.predict(...) accepts a 4D tensor of shape (batch, x, y, nb_channels), such as a Keras model.\n",
        "predictions_smooth = predict_img_with_smooth_windowing(\n",
        "    input_img,\n",
        "    window_size=patch_size,\n",
        "    subdivisions=2,  # Minimal amount of overlap for windowing. Must be an even number.\n",
        "    nb_classes=n_classes,\n",
        "    pred_func=(\n",
        "        lambda img_batch_subdiv: model.predict((img_batch_subdiv))\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "final_prediction = np.argmax(predictions_smooth, axis=2)\n",
        "\n",
        "#Save prediction and original mask for comparison\n",
        "plt.imsave('data/test_images/N-34-66-C-c-4-3.tif_segmented.jpg', final_prediction)\n",
        "plt.imsave('data/test_images/N-34-66-C-c-4-3.tif_mask.jpg', original_mask)\n",
        "###################\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.subplot(221)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(img)\n",
        "plt.subplot(222)\n",
        "plt.title('Testing Label')\n",
        "plt.imshow(original_mask)\n",
        "plt.subplot(223)\n",
        "plt.title('Prediction with smooth blending')\n",
        "plt.imshow(final_prediction)\n",
        "plt.show()\n",
        "\n",
        "#############################"
      ],
      "metadata": {
        "id": "t72L9TfwqxfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##smooth_tiled_predictions.py"
      ],
      "metadata": {
        "id": "tE7fnxcTqxhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gc\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import matplotlib.pyplot as plt\n",
        "    PLOT_PROGRESS = True\n",
        "    # See end of file for the rest of the __main__.\n",
        "else:\n",
        "    PLOT_PROGRESS = False\n",
        "\n",
        "\n",
        "def _spline_window(window_size, power=2):\n",
        "    \"\"\"\n",
        "    Squared spline (power=2) window function:\n",
        "    https://www.wolframalpha.com/input/?i=y%3Dx**2,+y%3D-(x-2)**2+%2B2,+y%3D(x-4)**2,+from+y+%3D+0+to+2\n",
        "    \"\"\"\n",
        "    intersection = int(window_size/4)\n",
        "    wind_outer = (abs(2*(scipy.signal.triang(window_size))) ** power)/2\n",
        "    wind_outer[intersection:-intersection] = 0\n",
        "\n",
        "    wind_inner = 1 - (abs(2*(scipy.signal.triang(window_size) - 1)) ** power)/2\n",
        "    wind_inner[:intersection] = 0\n",
        "    wind_inner[-intersection:] = 0\n",
        "\n",
        "    wind = wind_inner + wind_outer\n",
        "    wind = wind / np.average(wind)\n",
        "    return wind\n",
        "\n",
        "\n",
        "cached_2d_windows = dict()\n",
        "def _window_2D(window_size, power=2):\n",
        "    \"\"\"\n",
        "    Make a 1D window function, then infer and return a 2D window function.\n",
        "    Done with an augmentation, and self multiplication with its transpose.\n",
        "    Could be generalized to more dimensions.\n",
        "    \"\"\"\n",
        "    # Memoization\n",
        "    global cached_2d_windows\n",
        "    key = \"{}_{}\".format(window_size, power)\n",
        "    if key in cached_2d_windows:\n",
        "        wind = cached_2d_windows[key]\n",
        "    else:\n",
        "        wind = _spline_window(window_size, power)\n",
        "        wind = np.expand_dims(np.expand_dims(wind, 1), 1)      #SREENI: Changed from 3, 3, to 1, 1 \n",
        "        wind = wind * wind.transpose(1, 0, 2)\n",
        "        if PLOT_PROGRESS:\n",
        "            # For demo purpose, let's look once at the window:\n",
        "            plt.imshow(wind[:, :, 0], cmap=\"viridis\")\n",
        "            plt.title(\"2D Windowing Function for a Smooth Blending of \"\n",
        "                      \"Overlapping Patches\")\n",
        "            plt.show()\n",
        "        cached_2d_windows[key] = wind\n",
        "    return wind\n",
        "\n",
        "\n",
        "def _pad_img(img, window_size, subdivisions):\n",
        "    \"\"\"\n",
        "    Add borders to img for a \"valid\" border pattern according to \"window_size\" and\n",
        "    \"subdivisions\".\n",
        "    Image is an np array of shape (x, y, nb_channels).\n",
        "    \"\"\"\n",
        "    aug = int(round(window_size * (1 - 1.0/subdivisions)))\n",
        "    more_borders = ((aug, aug), (aug, aug), (0, 0))\n",
        "    ret = np.pad(img, pad_width=more_borders, mode='reflect')\n",
        "    # gc.collect()\n",
        "\n",
        "    if PLOT_PROGRESS:\n",
        "        # For demo purpose, let's look once at the window:\n",
        "        plt.imshow(ret)\n",
        "        plt.title(\"Padded Image for Using Tiled Prediction Patches\\n\"\n",
        "                  \"(notice the reflection effect on the padded borders)\")\n",
        "        plt.show()\n",
        "    return ret\n",
        "\n",
        "\n",
        "def _unpad_img(padded_img, window_size, subdivisions):\n",
        "    \"\"\"\n",
        "    Undo what's done in the `_pad_img` function.\n",
        "    Image is an np array of shape (x, y, nb_channels).\n",
        "    \"\"\"\n",
        "    aug = int(round(window_size * (1 - 1.0/subdivisions)))\n",
        "    ret = padded_img[\n",
        "        aug:-aug,\n",
        "        aug:-aug,\n",
        "        :\n",
        "    ]\n",
        "    # gc.collect()\n",
        "    return ret\n",
        "\n",
        "\n",
        "def _rotate_mirror_do(im):\n",
        "    \"\"\"\n",
        "    Duplicate an np array (image) of shape (x, y, nb_channels) 8 times, in order\n",
        "    to have all the possible rotations and mirrors of that image that fits the\n",
        "    possible 90 degrees rotations.\n",
        "    It is the D_4 (D4) Dihedral group:\n",
        "    https://en.wikipedia.org/wiki/Dihedral_group\n",
        "    \"\"\"\n",
        "    mirrs = []\n",
        "    mirrs.append(np.array(im))\n",
        "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=1))\n",
        "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=2))\n",
        "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=3))\n",
        "    im = np.array(im)[:, ::-1]\n",
        "    mirrs.append(np.array(im))\n",
        "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=1))\n",
        "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=2))\n",
        "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=3))\n",
        "    return mirrs\n",
        "\n",
        "\n",
        "def _rotate_mirror_undo(im_mirrs):\n",
        "    \"\"\"\n",
        "    merges a list of 8 np arrays (images) of shape (x, y, nb_channels) generated\n",
        "    from the `_rotate_mirror_do` function. Each images might have changed and\n",
        "    merging them implies to rotated them back in order and average things out.\n",
        "    It is the D_4 (D4) Dihedral group:\n",
        "    https://en.wikipedia.org/wiki/Dihedral_group\n",
        "    \"\"\"\n",
        "    origs = []\n",
        "    origs.append(np.array(im_mirrs[0]))\n",
        "    origs.append(np.rot90(np.array(im_mirrs[1]), axes=(0, 1), k=3))\n",
        "    origs.append(np.rot90(np.array(im_mirrs[2]), axes=(0, 1), k=2))\n",
        "    origs.append(np.rot90(np.array(im_mirrs[3]), axes=(0, 1), k=1))\n",
        "    origs.append(np.array(im_mirrs[4])[:, ::-1])\n",
        "    origs.append(np.rot90(np.array(im_mirrs[5]), axes=(0, 1), k=3)[:, ::-1])\n",
        "    origs.append(np.rot90(np.array(im_mirrs[6]), axes=(0, 1), k=2)[:, ::-1])\n",
        "    origs.append(np.rot90(np.array(im_mirrs[7]), axes=(0, 1), k=1)[:, ::-1])\n",
        "    return np.mean(origs, axis=0)\n",
        "\n",
        "\n",
        "def _windowed_subdivs(padded_img, window_size, subdivisions, nb_classes, pred_func):\n",
        "    \"\"\"\n",
        "    Create tiled overlapping patches.\n",
        "    Returns:\n",
        "        5D numpy array of shape = (\n",
        "            nb_patches_along_X,\n",
        "            nb_patches_along_Y,\n",
        "            patches_resolution_along_X,\n",
        "            patches_resolution_along_Y,\n",
        "            nb_output_channels\n",
        "        )\n",
        "    Note:\n",
        "        patches_resolution_along_X == patches_resolution_along_Y == window_size\n",
        "    \"\"\"\n",
        "    WINDOW_SPLINE_2D = _window_2D(window_size=window_size, power=2)\n",
        "\n",
        "    step = int(window_size/subdivisions)\n",
        "    padx_len = padded_img.shape[0]\n",
        "    pady_len = padded_img.shape[1]\n",
        "    subdivs = []\n",
        "\n",
        "    for i in range(0, padx_len-window_size+1, step):\n",
        "        subdivs.append([])\n",
        "        for j in range(0, pady_len-window_size+1, step):            #SREENI: Changed padx to pady (Bug in original code)\n",
        "            patch = padded_img[i:i+window_size, j:j+window_size, :]\n",
        "            subdivs[-1].append(patch)\n",
        "\n",
        "    # Here, `gc.collect()` clears RAM between operations.\n",
        "    # It should run faster if they are removed, if enough memory is available.\n",
        "    gc.collect()\n",
        "    subdivs = np.array(subdivs)\n",
        "    gc.collect()\n",
        "    a, b, c, d, e = subdivs.shape\n",
        "    subdivs = subdivs.reshape(a * b, c, d, e)\n",
        "    gc.collect()\n",
        "\n",
        "    subdivs = pred_func(subdivs)\n",
        "    gc.collect()\n",
        "    subdivs = np.array([patch * WINDOW_SPLINE_2D for patch in subdivs])\n",
        "    gc.collect()\n",
        "\n",
        "    # Such 5D array:\n",
        "    subdivs = subdivs.reshape(a, b, c, d, nb_classes)\n",
        "    gc.collect()\n",
        "\n",
        "    return subdivs\n",
        "\n",
        "\n",
        "def _recreate_from_subdivs(subdivs, window_size, subdivisions, padded_out_shape):\n",
        "    \"\"\"\n",
        "    Merge tiled overlapping patches smoothly.\n",
        "    \"\"\"\n",
        "    step = int(window_size/subdivisions)\n",
        "    padx_len = padded_out_shape[0]\n",
        "    pady_len = padded_out_shape[1]\n",
        "\n",
        "    y = np.zeros(padded_out_shape)\n",
        "\n",
        "    a = 0\n",
        "    for i in range(0, padx_len-window_size+1, step):\n",
        "        b = 0\n",
        "        for j in range(0, pady_len-window_size+1, step):                #SREENI: Changed padx to pady (Bug in original code)\n",
        "            windowed_patch = subdivs[a, b]\n",
        "            y[i:i+window_size, j:j+window_size] = y[i:i+window_size, j:j+window_size] + windowed_patch\n",
        "            b += 1\n",
        "        a += 1\n",
        "    return y / (subdivisions ** 2)\n",
        "\n",
        "\n",
        "def predict_img_with_smooth_windowing(input_img, window_size, subdivisions, nb_classes, pred_func):\n",
        "    \"\"\"\n",
        "    Apply the `pred_func` function to square patches of the image, and overlap\n",
        "    the predictions to merge them smoothly.\n",
        "    See 6th, 7th and 8th idea here:\n",
        "    http://blog.kaggle.com/2017/05/09/dstl-satellite-imagery-competition-3rd-place-winners-interview-vladimir-sergey/\n",
        "    \"\"\"\n",
        "    pad = _pad_img(input_img, window_size, subdivisions)\n",
        "    pads = _rotate_mirror_do(pad)\n",
        "\n",
        "    # Note that the implementation could be more memory-efficient by merging\n",
        "    # the behavior of `_windowed_subdivs` and `_recreate_from_subdivs` into\n",
        "    # one loop doing in-place assignments to the new image matrix, rather than\n",
        "    # using a temporary 5D array.\n",
        "\n",
        "    # It would also be possible to allow different (and impure) window functions\n",
        "    # that might not tile well. Adding their weighting to another matrix could\n",
        "    # be done to later normalize the predictions correctly by dividing the whole\n",
        "    # reconstructed thing by this matrix of weightings - to normalize things\n",
        "    # back from an impure windowing function that would have badly weighted\n",
        "    # windows.\n",
        "\n",
        "    # For example, since the U-net of Kaggle's DSTL satellite imagery feature\n",
        "    # prediction challenge's 3rd place winners use a different window size for\n",
        "    # the input and output of the neural net's patches predictions, it would be\n",
        "    # possible to fake a full-size window which would in fact just have a narrow\n",
        "    # non-zero dommain. This may require to augment the `subdivisions` argument\n",
        "    # to 4 rather than 2.\n",
        "\n",
        "    res = []\n",
        "    for pad in tqdm(pads):\n",
        "        # For every rotation:\n",
        "        sd = _windowed_subdivs(pad, window_size, subdivisions, nb_classes, pred_func)\n",
        "        one_padded_result = _recreate_from_subdivs(\n",
        "            sd, window_size, subdivisions,\n",
        "            padded_out_shape=list(pad.shape[:-1])+[nb_classes])\n",
        "\n",
        "        res.append(one_padded_result)\n",
        "\n",
        "    # Merge after rotations:\n",
        "    padded_results = _rotate_mirror_undo(res)\n",
        "\n",
        "    prd = _unpad_img(padded_results, window_size, subdivisions)\n",
        "\n",
        "    prd = prd[:input_img.shape[0], :input_img.shape[1], :]\n",
        "\n",
        "    if PLOT_PROGRESS:\n",
        "        plt.imshow(prd)\n",
        "        plt.title(\"Smoothly Merged Patches that were Tiled Tighter\")\n",
        "        plt.show()\n",
        "    return prd\n"
      ],
      "metadata": {
        "id": "nbW-hJHXqxlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##training_landcover_keras_augmentation_V2.0.py"
      ],
      "metadata": {
        "id": "eNAh5b4CqxoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dataset from : https://landcover.ai/\n",
        "Dataset description: https://arxiv.org/pdf/2005.02264.pdf\n",
        "labels:\n",
        "    0: Unlabeled background \n",
        "    1: Buildings\n",
        "    2: Woodlands\n",
        "    3: Water\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import segmentation_models as sm\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "import random\n",
        "\n",
        "# tf.compat.v1.disable_eager_execution() #in case the model gets very slow, may be due to a bug in TF2.0. Uncomment this. \n",
        "#https://github.com/tensorflow/tensorflow/issues/33024\n",
        "\n",
        "#Also check this in case you notice training to be getting increasingly slow each epoch.\n",
        "# https://stackoverflow.com/questions/53683164/keras-occupies-an-indefinitely-increasing-amount-of-memory-for-each-epoch\n",
        "\n",
        "################################################################\n",
        "#Get an understanding by looking at a few random images and masks \n",
        "\n",
        "train_img_dir = \"data/data_for_keras_aug/train_images/train/\"\n",
        "train_mask_dir = \"data/data_for_keras_aug/train_masks/train/\"\n",
        "\n",
        "img_list = os.listdir(train_img_dir)\n",
        "msk_list = os.listdir(train_mask_dir)\n",
        "\n",
        "num_images = len(os.listdir(train_img_dir))\n",
        "\n",
        "\n",
        "img_num = random.randint(0, num_images-1)\n",
        "\n",
        "img_for_plot = cv2.imread(train_img_dir+img_list[img_num], 1)\n",
        "img_for_plot = cv2.cvtColor(img_for_plot, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "mask_for_plot =cv2.imread(train_mask_dir+msk_list[img_num], 0)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(121)\n",
        "plt.imshow(img_for_plot)\n",
        "plt.title('Image')\n",
        "plt.subplot(122)\n",
        "plt.imshow(mask_for_plot, cmap='gray')\n",
        "plt.title('Mask')\n",
        "plt.show()\n",
        "\n",
        "################################################################\n",
        "\n",
        "# Define Generator for images and masks so we can read them directly from the drive. \n",
        "\n",
        "seed=24\n",
        "batch_size= 16\n",
        "n_classes=4\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#Use this to preprocess input for transfer learning\n",
        "BACKBONE = 'resnet34'\n",
        "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
        "\n",
        "#Define a function to perform additional preprocessing after datagen.\n",
        "#For example, scale images, convert masks to categorical, etc. \n",
        "def preprocess_data(img, mask, num_class):\n",
        "    #Scale images\n",
        "    img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n",
        "    img = preprocess_input(img)  #Preprocess based on the pretrained backbone...\n",
        "    #Convert mask to one-hot\n",
        "    mask = to_categorical(mask, num_class)\n",
        "      \n",
        "    return (img,mask)\n",
        "\n",
        "#Define the generator.\n",
        "#We are not doing any rotation or zoom to make sure mask values are not interpolated.\n",
        "#It is important to keep pixel values in mask as 0, 1, 2, 3, .....\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "def trainGenerator(train_img_path, train_mask_path, num_class):\n",
        "    \n",
        "    img_data_gen_args = dict(horizontal_flip=True,\n",
        "                      vertical_flip=True,\n",
        "                      fill_mode='reflect')\n",
        "    \n",
        "    image_datagen = ImageDataGenerator(**img_data_gen_args)\n",
        "    mask_datagen = ImageDataGenerator(**img_data_gen_args)\n",
        "    \n",
        "    image_generator = image_datagen.flow_from_directory(\n",
        "        train_img_path,\n",
        "        class_mode = None,\n",
        "        batch_size = batch_size,\n",
        "        seed = seed)\n",
        "    \n",
        "    mask_generator = mask_datagen.flow_from_directory(\n",
        "        train_mask_path,\n",
        "        class_mode = None,\n",
        "        color_mode = 'grayscale',\n",
        "        batch_size = batch_size,\n",
        "        seed = seed)\n",
        "    \n",
        "    train_generator = zip(image_generator, mask_generator)\n",
        "    \n",
        "    for (img, mask) in train_generator:\n",
        "        img, mask = preprocess_data(img, mask, num_class)\n",
        "        yield (img, mask)\n",
        "\n",
        "\n",
        "train_img_path = \"data/data_for_keras_aug/train_images/\"\n",
        "train_mask_path = \"data/data_for_keras_aug/train_masks/\"\n",
        "train_img_gen = trainGenerator(train_img_path, train_mask_path, num_class=4)\n",
        "\n",
        "val_img_path = \"data/data_for_keras_aug/val_images/\"\n",
        "val_mask_path = \"data/data_for_keras_aug/val_masks/\"\n",
        "val_img_gen = trainGenerator(val_img_path, val_mask_path, num_class=4)\n",
        "\n",
        "#Make sure the generator is working and that images and masks are indeed lined up. \n",
        "#Verify generator.... In python 3 next() is renamed as __next__()\n",
        "x, y = train_img_gen.__next__()\n",
        "\n",
        "for i in range(0,3):\n",
        "    image = x[i]\n",
        "    mask = np.argmax(y[i], axis=2)\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(image)\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "x_val, y_val = val_img_gen.__next__()\n",
        "\n",
        "for i in range(0,3):\n",
        "    image = x_val[i]\n",
        "    mask = np.argmax(y_val[i], axis=2)\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(image)\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "###########################################################################\n",
        "#Define the model metrcis and load model. \n",
        "\n",
        "num_train_imgs = len(os.listdir('data/data_for_keras_aug/train_images/train/'))\n",
        "num_val_images = len(os.listdir('data/data_for_keras_aug/val_images/val/'))\n",
        "steps_per_epoch = num_train_imgs//batch_size\n",
        "val_steps_per_epoch = num_val_images//batch_size\n",
        "\n",
        "\n",
        "IMG_HEIGHT = x.shape[1]\n",
        "IMG_WIDTH  = x.shape[2]\n",
        "IMG_CHANNELS = x.shape[3]\n",
        "\n",
        "n_classes=4\n",
        "\n",
        "#############################################################################\n",
        "#Use transfer learning using pretrained encoder in the U-Net\n",
        "#(make sure you uncomment the preprocess_input part in the\n",
        "# preprocess_data function above)\n",
        "################################################################\n",
        "#Define the model\n",
        "# define model\n",
        "model = sm.Unet(BACKBONE, encoder_weights='imagenet', \n",
        "                input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
        "                classes=n_classes, activation='softmax')\n",
        "model.compile('Adam', loss=sm.losses.categorical_focal_jaccard_loss, metrics=[sm.metrics.iou_score])\n",
        "\n",
        "#Other losses to try: categorical_focal_dice_loss, cce_jaccard_loss, cce_dice_loss, categorical_focal_loss\n",
        "\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "print(model.summary())\n",
        "print(model.input_shape)\n",
        "#Fit the model\n",
        "#history = model.fit(my_generator, validation_data=validation_datagen, steps_per_epoch=len(X_train) // 16, validation_steps=len(X_train) // 16, epochs=100)\n",
        "#Train the model. \n",
        "history=model.fit(train_img_gen,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          epochs=25,\n",
        "          verbose=1,\n",
        "          validation_data=val_img_gen,\n",
        "          validation_steps=val_steps_per_epoch)\n",
        "\n",
        "model.save('landcover_25_epochs_RESNET_backbone_batch16.hdf5')\n",
        "\n",
        "##################################################################\n",
        "#plot the training and validation IoU and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history.history['iou_score']\n",
        "val_acc = history.history['val_iou_score']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training IoU')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation IoU')\n",
        "plt.title('Training and validation IoU')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IoU')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#####################################################\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model(\"landcover_25_epochs_RESNET_backbone_batch16.hdf5\", compile=False)\n",
        "\n",
        "#batch_size=32 #Check IoU for a batch of images\n",
        "\n",
        "#Test generator using validation data.\n",
        "\n",
        "test_image_batch, test_mask_batch = val_img_gen.__next__()\n",
        "\n",
        "#Convert categorical to integer for visualization and IoU calculation\n",
        "test_mask_batch_argmax = np.argmax(test_mask_batch, axis=3) \n",
        "test_pred_batch = model.predict(test_image_batch)\n",
        "test_pred_batch_argmax = np.argmax(test_pred_batch, axis=3)\n",
        "\n",
        "n_classes = 4\n",
        "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
        "IOU_keras.update_state(test_pred_batch_argmax, test_mask_batch_argmax)\n",
        "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
        "\n",
        "#######################################################\n",
        "#View a few images, masks and corresponding predictions. \n",
        "img_num = random.randint(0, test_image_batch.shape[0]-1)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_image_batch[img_num])\n",
        "plt.subplot(232)\n",
        "plt.title('Testing Label')\n",
        "plt.imshow(test_mask_batch_argmax[img_num])\n",
        "plt.subplot(233)\n",
        "plt.title('Prediction on test image')\n",
        "plt.imshow(test_pred_batch_argmax[img_num])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l1xwijo7qxrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JfUWa5m3qx6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7oMS8ORIqx9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D0UrTx9zx6Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZcIuKauRx6Tw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}