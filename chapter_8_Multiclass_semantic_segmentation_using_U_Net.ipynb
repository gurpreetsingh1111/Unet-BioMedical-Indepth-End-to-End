{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_7 Multiclass semantic segmentation using U-Net.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Standard Unet\n",
        "Model not compiled here, instead will be done externally to make it\n",
        "easy to test various loss functions and optimizers. \n",
        "\"\"\"\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "################################################################\n",
        "def multi_unet_model(n_classes=4, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=1):\n",
        "#Build the model\n",
        "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
        "    s = inputs\n",
        "\n",
        "    #Contraction path\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
        "    c1 = Dropout(0.1)(c1)\n",
        "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "    \n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = Dropout(0.1)(c2)\n",
        "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "     \n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "     \n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = Dropout(0.2)(c4)\n",
        "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "     \n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "    c5 = Dropout(0.3)(c5)\n",
        "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "    \n",
        "    #Expansive path \n",
        "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = Dropout(0.2)(c6)\n",
        "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "     \n",
        "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = Dropout(0.2)(c7)\n",
        "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "     \n",
        "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "    c8 = Dropout(0.1)(c8)\n",
        "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "     \n",
        "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "    c9 = Dropout(0.1)(c9)\n",
        "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "     \n",
        "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
        "     \n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    \n",
        "    #NOTE: Compile the model in the main program to make it easy to test with various loss functions\n",
        "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    #model.summary()\n",
        "    \n",
        "    return model\n",
        " "
      ],
      "metadata": {
        "id": "ZIcj3Dpv3BMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Multiclass semantic segmentation using U-Net\n",
        "Including segmenting large images by dividing them into smaller patches \n",
        "and stiching them back\n",
        "To annotate images and generate labels, you can use APEER (for free):\n",
        "www.apeer.com \n",
        "\"\"\"\n",
        "\n",
        "from simple_multi_unet_model import multi_unet_model #Uses softmax \n",
        "\n",
        "from keras.utils import normalize\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "#Resizing images, if needed\n",
        "SIZE_X = 128 \n",
        "SIZE_Y = 128\n",
        "n_classes=4 #Number of classes for segmentation\n",
        "\n",
        "#Capture training image info as a list\n",
        "train_images = []\n",
        "\n",
        "for directory_path in glob.glob(\"128_patches/images/\"):\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
        "        img = cv2.imread(img_path, 0)       \n",
        "        #img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
        "        train_images.append(img)\n",
        "       \n",
        "#Convert list to array for machine learning processing        \n",
        "train_images = np.array(train_images)\n",
        "\n",
        "#Capture mask/label info as a list\n",
        "train_masks = [] \n",
        "for directory_path in glob.glob(\"128_patches/masks/\"):\n",
        "    for mask_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
        "        mask = cv2.imread(mask_path, 0)       \n",
        "        #mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
        "        train_masks.append(mask)\n",
        "        \n",
        "#Convert list to array for machine learning processing          \n",
        "train_masks = np.array(train_masks)\n",
        "\n",
        "###############################################\n",
        "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "n, h, w = train_masks.shape\n",
        "train_masks_reshaped = train_masks.reshape(-1,1)\n",
        "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
        "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "np.unique(train_masks_encoded_original_shape)\n",
        "\n",
        "#################################################\n",
        "train_images = np.expand_dims(train_images, axis=3)\n",
        "train_images = normalize(train_images, axis=1)\n",
        "\n",
        "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
        "\n",
        "#Create a subset of data for quick testing\n",
        "#Picking 10% for testing and remaining for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
        "\n",
        "#Further split training data t a smaller subset for quick testing of models\n",
        "X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.2, random_state = 0)\n",
        "\n",
        "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \n",
        "\n",
        "from keras.utils import to_categorical\n",
        "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
        "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
        "\n",
        "\n",
        "\n",
        "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
        "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))\n",
        "\n",
        "\n",
        "\n",
        "###############################################################\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(train_masks_reshaped_encoded),\n",
        "                                                 train_masks_reshaped_encoded)\n",
        "print(\"Class weights are...:\", class_weights)\n",
        "\n",
        "\n",
        "IMG_HEIGHT = X_train.shape[1]\n",
        "IMG_WIDTH  = X_train.shape[2]\n",
        "IMG_CHANNELS = X_train.shape[3]\n",
        "\n",
        "def get_model():\n",
        "    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n",
        "\n",
        "model = get_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#If starting with pre-trained weights. \n",
        "#model.load_weights('???.hdf5')\n",
        "\n",
        "history = model.fit(X_train, y_train_cat, \n",
        "                    batch_size = 16, \n",
        "                    verbose=1, \n",
        "                    epochs=50, \n",
        "                    validation_data=(X_test, y_test_cat), \n",
        "                    #class_weight=class_weights,\n",
        "                    shuffle=False)\n",
        "                    \n",
        "\n",
        "\n",
        "model.save('test.hdf5')\n",
        "#model.save('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')\n",
        "############################################################\n",
        "#Evaluate the model\n",
        "\t# evaluate model\n",
        "_, acc = model.evaluate(X_test, y_test_cat)\n",
        "print(\"Accuracy is = \", (acc * 100.0), \"%\")\n",
        "\n",
        "\n",
        "###\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "##################################\n",
        "#model = get_model()\n",
        "model.load_weights('sandstone_50_epochs_catXentropy_acc.hdf5')  \n",
        "#model.load_weights('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')  \n",
        "\n",
        "#IOU\n",
        "y_pred=model.predict(X_test)\n",
        "y_pred_argmax=np.argmax(y_pred, axis=3)\n",
        "\n",
        "##################################################\n",
        "\n",
        "#Using built in keras function\n",
        "from keras.metrics import MeanIoU\n",
        "n_classes = 4\n",
        "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
        "IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)\n",
        "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
        "\n",
        "\n",
        "#To calculate I0U for each class...\n",
        "values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
        "print(values)\n",
        "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
        "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
        "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
        "class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
        "\n",
        "print(\"IoU for class1 is: \", class1_IoU)\n",
        "print(\"IoU for class2 is: \", class2_IoU)\n",
        "print(\"IoU for class3 is: \", class3_IoU)\n",
        "print(\"IoU for class4 is: \", class4_IoU)\n",
        "\n",
        "plt.imshow(train_images[0, :,:,0], cmap='gray')\n",
        "plt.imshow(train_masks[0], cmap='gray')\n",
        "#######################################################################\n",
        "#Predict on a few images\n",
        "#model = get_model()\n",
        "#model.load_weights('???.hdf5')  \n",
        "import random\n",
        "test_img_number = random.randint(0, len(X_test))\n",
        "test_img = X_test[test_img_number]\n",
        "ground_truth=y_test[test_img_number]\n",
        "test_img_norm=test_img[:,:,0][:,:,None]\n",
        "test_img_input=np.expand_dims(test_img_norm, 0)\n",
        "prediction = (model.predict(test_img_input))\n",
        "predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img[:,:,0], cmap='gray')\n",
        "plt.subplot(232)\n",
        "plt.title('Testing Label')\n",
        "plt.imshow(ground_truth[:,:,0], cmap='jet')\n",
        "plt.subplot(233)\n",
        "plt.title('Prediction on test image')\n",
        "plt.imshow(predicted_img, cmap='jet')\n",
        "plt.show()\n",
        "\n",
        "#####################################################################\n",
        "\n",
        "#Predict on large image\n",
        "\n",
        "#Apply a trained model on large image\n",
        "\n",
        "from patchify import patchify, unpatchify# unpatchify means put them together\n",
        "\n",
        "large_image = cv2.imread('large_images/large_image.tif', 0)# (768,768)\n",
        "#This will split the image into small images of shape [3,3]\n",
        "patches = patchify(large_image, (128, 128), step=128)  #Step=256 for 256 patches means no overlap\n",
        "\n",
        "predicted_patches = []\n",
        "for i in range(patches.shape[0]):\n",
        "    for j in range(patches.shape[1]):\n",
        "        print(i,j)\n",
        "        \n",
        "        single_patch = patches[i,j,:,:]       \n",
        "        single_patch_norm = np.expand_dims(normalize(np.array(single_patch), axis=1),2)\n",
        "        single_patch_input=np.expand_dims(single_patch_norm, 0)\n",
        "        single_patch_prediction = (model.predict(single_patch_input))\n",
        "        single_patch_predicted_img=np.argmax(single_patch_prediction, axis=3)[0,:,:]\n",
        "\n",
        "        predicted_patches.append(single_patch_predicted_img)\n",
        "\n",
        "# Takes the large images break into the patches each patch process it just like we processed our original traning data predit on it and get the predictions organized again and back into the original shape and put them back togeather \n",
        "predicted_patches = np.array(predicted_patches)# (36,128,128)\n",
        "\n",
        "predicted_patches_reshaped = np.reshape(predicted_patches, (patches.shape[0], patches.shape[1], 128,128) )# (6,6,128,128)\n",
        "\n",
        "reconstructed_image = unpatchify(predicted_patches_reshaped, large_image.shape)#(768,768)\n",
        "plt.imshow(reconstructed_image, cmap='gray')\n",
        "#plt.imsave('data/results/segm.jpg', reconstructed_image, cmap='gray')\n",
        "\n",
        "plt.hist(reconstructed_image.flatten())  #Threshold everything above 0\n",
        "\n",
        "# final_prediction = (reconstructed_image > 0.01).astype(np.uint8)\n",
        "# plt.imshow(final_prediction)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(221)\n",
        "plt.title('Large Image')\n",
        "plt.imshow(large_image, cmap='gray')\n",
        "plt.subplot(222)\n",
        "plt.title('Prediction of large Image')\n",
        "plt.imshow(reconstructed_image, cmap='jet')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "luU-VKZE3BPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v9NvSTzH3BTE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}