{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter_12_Improving semantic segmentation (U-Net) performance via ensemble of multiple trained networks",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import segmentation_models as sm\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import keras \n",
        "\n",
        "from keras.utils import normalize\n",
        "from keras.metrics import MeanIoU\n",
        "\n",
        "\n",
        "#Resizing images, if needed\n",
        "SIZE_X = 128 \n",
        "SIZE_Y = 128\n",
        "n_classes=4 #Number of classes for segmentation\n",
        "\n",
        "#Capture training image info as a list\n",
        "train_images = []\n",
        "\n",
        "for directory_path in glob.glob(\"128_patches/images/\"):\n",
        "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
        "        img = cv2.imread(img_path, 1)       \n",
        "        #img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
        "        train_images.append(img)\n",
        "       \n",
        "#Convert list to array for machine learning processing        \n",
        "train_images = np.array(train_images)\n",
        "\n",
        "#Capture mask/label info as a list\n",
        "train_masks = [] \n",
        "for directory_path in glob.glob(\"128_patches/masks/\"):\n",
        "    for mask_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
        "        mask = cv2.imread(mask_path, 0)       \n",
        "        #mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
        "        train_masks.append(mask)\n",
        "        \n",
        "#Convert list to array for machine learning processing          \n",
        "train_masks = np.array(train_masks)\n",
        "\n",
        "###############################################\n",
        "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "n, h, w = train_masks.shape\n",
        "train_masks_reshaped = train_masks.reshape(-1,1)\n",
        "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
        "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "np.unique(train_masks_encoded_original_shape)\n",
        "\n",
        "#################################################\n",
        "#train_images = np.expand_dims(train_images, axis=3)\n",
        "#train_images = normalize(train_images, axis=1)\n",
        "\n",
        "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
        "\n",
        "#Create a subset of data for quick testing\n",
        "#Picking 10% for testing and remaining for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
        "\n",
        "#Further split training data t a smaller subset for quick testing of models\n",
        "X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.5, random_state = 0)\n",
        "\n",
        "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \n",
        "\n",
        "from keras.utils import to_categorical\n",
        "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
        "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
        "\n",
        "\n",
        "\n",
        "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
        "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))\n",
        "\n",
        "######################################################\n",
        "#Reused parameters in all models\n",
        "\n",
        "n_classes=4\n",
        "activation='softmax'\n",
        "\n",
        "LR = 0.0001\n",
        "optim = keras.optimizers.Adam(LR)\n",
        "\n",
        "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
        "# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
        "dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.25, 0.25, 0.25, 0.25])) \n",
        "focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "total_loss = dice_loss + (1 * focal_loss)\n",
        "\n",
        "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
        "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
        "\n",
        "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
        "\n",
        "\n",
        "########################################################################\n",
        "###Model 1\n",
        "BACKBONE1 = 'resnet34'\n",
        "preprocess_input1 = sm.get_preprocessing(BACKBONE1)\n",
        "\n",
        "# preprocess input\n",
        "X_train1 = preprocess_input1(X_train)\n",
        "X_test1 = preprocess_input1(X_test)\n",
        "\n",
        "# define model\n",
        "model1 = sm.Unet(BACKBONE1, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model1.compile(optim, total_loss, metrics=metrics)\n",
        "\n",
        "#model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "\n",
        "print(model1.summary())\n",
        "\n",
        "\n",
        "history1=model1.fit(X_train1, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test1, y_test_cat))\n",
        "\n",
        "\n",
        "model1.save('res34_backbone_50epochs.hdf5')\n",
        "############################################################\n",
        "###Model 2\n",
        "\n",
        "BACKBONE2 = 'inceptionv3'\n",
        "preprocess_input2 = sm.get_preprocessing(BACKBONE2)\n",
        "\n",
        "# preprocess input\n",
        "X_train2 = preprocess_input2(X_train)\n",
        "X_test2 = preprocess_input2(X_test)\n",
        "\n",
        "# define model\n",
        "model2 = sm.Unet(BACKBONE2, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model2.compile(optim, total_loss, metrics)\n",
        "#model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "\n",
        "\n",
        "print(model2.summary())\n",
        "\n",
        "\n",
        "history2=model2.fit(X_train2, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test2, y_test_cat))\n",
        "\n",
        "\n",
        "model2.save('inceptionv3_backbone_50epochs.hdf5')\n",
        "\n",
        "#####################################################\n",
        "###Model 3\n",
        "\n",
        "BACKBONE3 = 'vgg16'\n",
        "preprocess_input3 = sm.get_preprocessing(BACKBONE3)\n",
        "\n",
        "# preprocess input\n",
        "X_train3 = preprocess_input3(X_train)\n",
        "X_test3 = preprocess_input3(X_test)\n",
        "\n",
        "\n",
        "# define model\n",
        "model3 = sm.Unet(BACKBONE3, encoder_weights='imagenet', classes=n_classes, activation=activation)\n",
        "\n",
        "# compile keras model with defined optimozer, loss and metrics\n",
        "model3.compile(optim, total_loss, metrics)\n",
        "#model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
        "\n",
        "\n",
        "print(model3.summary())\n",
        "\n",
        "history3=model3.fit(X_train3, \n",
        "          y_train_cat,\n",
        "          batch_size=8, \n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test3, y_test_cat))\n",
        "\n",
        "\n",
        "model3.save('vgg19_backbone_50epochs.hdf5')\n",
        "\n",
        "\n",
        "##########################################################\n",
        "\n",
        "###\n",
        "#plot the training and validation accuracy and loss at each epoch\n",
        "loss = history1.history['loss']\n",
        "val_loss = history1.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history1.history['iou_score']\n",
        "val_acc = history1.history['val_iou_score']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation IOU')\n",
        "plt.title('Training and validation IOU')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IOU')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#####################################################\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "#Set compile=False as we are not loading it for training, only for prediction.\n",
        "model1 = load_model('saved_models/res34_backbone_50epochs.hdf5', compile=False)\n",
        "model2 = load_model('saved_models/inceptionv3_backbone_50epochs.hdf5', compile=False)\n",
        "model3 = load_model('saved_models/vgg19_backbone_50epochs.hdf5', compile=False)\n",
        "\n",
        "#Weighted average ensemble\n",
        "models = [model1, model2, model3]\n",
        "#preds = [model.predict(X_test) for model in models]\n",
        "\n",
        "pred1 = model1.predict(X_test1)\n",
        "pred2 = model2.predict(X_test2)\n",
        "pred3 = model3.predict(X_test3)\n",
        "\n",
        "preds=np.array([pred1, pred2, pred3])\n",
        "\n",
        "#preds=np.array(preds)\n",
        "weights = [0.3, 0.5, 0.2]\n",
        "\n",
        "#Use tensordot to sum the products of all elements over specified axes.\n",
        "weighted_preds = np.tensordot(preds, weights, axes=((0),(0)))\n",
        "weighted_ensemble_prediction = np.argmax(weighted_preds, axis=3)\n",
        "\n",
        "y_pred1_argmax=np.argmax(pred1, axis=3)\n",
        "y_pred2_argmax=np.argmax(pred2, axis=3)\n",
        "y_pred3_argmax=np.argmax(pred3, axis=3)\n",
        "\n",
        "\n",
        "#Using built in keras function\n",
        "n_classes = 4\n",
        "IOU1 = MeanIoU(num_classes=n_classes)  \n",
        "IOU2 = MeanIoU(num_classes=n_classes)  \n",
        "IOU3 = MeanIoU(num_classes=n_classes)  \n",
        "IOU_weighted = MeanIoU(num_classes=n_classes)  \n",
        "\n",
        "IOU1.update_state(y_test[:,:,:,0], y_pred1_argmax)\n",
        "IOU2.update_state(y_test[:,:,:,0], y_pred2_argmax)\n",
        "IOU3.update_state(y_test[:,:,:,0], y_pred3_argmax)\n",
        "IOU_weighted.update_state(y_test[:,:,:,0], weighted_ensemble_prediction)\n",
        "\n",
        "\n",
        "print('IOU Score for model1 = ', IOU1.result().numpy())\n",
        "print('IOU Score for model2 = ', IOU2.result().numpy())\n",
        "print('IOU Score for model3 = ', IOU3.result().numpy())\n",
        "print('IOU Score for weighted average ensemble = ', IOU_weighted.result().numpy())\n",
        "###########################################\n",
        "#Grid search for the best combination of w1, w2, w3 that gives maximum acuracy\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame([])\n",
        "\n",
        "for w1 in range(0, 4):\n",
        "    for w2 in range(0,4):\n",
        "        for w3 in range(0,4):\n",
        "            wts = [w1/10.,w2/10.,w3/10.]\n",
        "            \n",
        "            IOU_wted = MeanIoU(num_classes=n_classes) \n",
        "            wted_preds = np.tensordot(preds, wts, axes=((0),(0)))\n",
        "            wted_ensemble_pred = np.argmax(wted_preds, axis=3)\n",
        "            IOU_wted.update_state(y_test[:,:,:,0], wted_ensemble_pred)\n",
        "            print(\"Now predciting for weights :\", w1/10., w2/10., w3/10., \" : IOU = \", IOU_wted.result().numpy())\n",
        "            df = df.append(pd.DataFrame({'wt1':wts[0],'wt2':wts[1], \n",
        "                                         'wt3':wts[2], 'IOU': IOU_wted.result().numpy()}, index=[0]), ignore_index=True)\n",
        "            \n",
        "max_iou_row = df.iloc[df['IOU'].idxmax()]\n",
        "print(\"Max IOU of \", max_iou_row[3], \" obained with w1=\", max_iou_row[0],\n",
        "      \" w2=\", max_iou_row[1], \" and w3=\", max_iou_row[2])         \n",
        "\n",
        "\n",
        "#############################################################\n",
        "opt_weights = [max_iou_row[0], max_iou_row[1], max_iou_row[2]]\n",
        "\n",
        "#Use tensordot to sum the products of all elements over specified axes.\n",
        "opt_weighted_preds = np.tensordot(preds, opt_weights, axes=((0),(0)))\n",
        "opt_weighted_ensemble_prediction = np.argmax(opt_weighted_preds, axis=3)\n",
        "#######################################################\n",
        "#Predict on a few images\n",
        "\n",
        "import random\n",
        "test_img_number = random.randint(0, len(X_test))\n",
        "test_img = X_test[test_img_number]\n",
        "ground_truth=y_test[test_img_number]\n",
        "test_img_norm=test_img[:,:,:]\n",
        "test_img_input=np.expand_dims(test_img_norm, 0)\n",
        "\n",
        "#Weighted average ensemble\n",
        "models = [model1, model2, model3]\n",
        "\n",
        "test_img_input1 = preprocess_input1(test_img_input)\n",
        "test_img_input2 = preprocess_input2(test_img_input)\n",
        "test_img_input3 = preprocess_input3(test_img_input)\n",
        "\n",
        "test_pred1 = model1.predict(test_img_input1)\n",
        "test_pred2 = model2.predict(test_img_input2)\n",
        "test_pred3 = model3.predict(test_img_input3)\n",
        "\n",
        "test_preds=np.array([test_pred1, test_pred2, test_pred3])\n",
        "\n",
        "#Use tensordot to sum the products of all elements over specified axes.\n",
        "weighted_test_preds = np.tensordot(test_preds, opt_weights, axes=((0),(0)))\n",
        "weighted_ensemble_test_prediction = np.argmax(weighted_test_preds, axis=3)[0,:,:]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img[:,:,0], cmap='gray')\n",
        "plt.subplot(232)\n",
        "plt.title('Testing Label')\n",
        "plt.imshow(ground_truth[:,:,0], cmap='jet')\n",
        "plt.subplot(233)\n",
        "plt.title('Prediction on test image')\n",
        "plt.imshow(weighted_ensemble_test_prediction, cmap='jet')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ES0R1wMDsy7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "enwgM7Rksy-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}